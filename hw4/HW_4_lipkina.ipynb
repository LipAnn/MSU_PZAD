{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание №4\n",
    "\n",
    "**Выполнила: Липкина Анна**\n",
    "    \n",
    "**517 группа**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание: Каким распределениям какие функции потерь соответствуют?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $a(x, w)$ --- алгоритм, зависящий от параметров $w$ и применяемый к $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известно, что следующие задачи эквивалентны:\n",
    "\n",
    "**1)** $$ p(y|a(x, w)) = \\mathcal{N}(y| a(x, w), \\sigma^2) \\Rightarrow \\prod\\limits_{i=1}^n p(y_i|a(x_i, w)) \\to \\max\\limits_{w} \\text{(ММП)} $$\n",
    "$$\\Leftrightarrow \\sum\\limits_{i=1}^n(y_i - a(x_i, w))^2 \\to \\min\\limits_{w} \\text{(MSE)}$$\n",
    "\n",
    "**2)** $$ p(y|a(x, w)) = Laplace(y|a(x, w), \\alpha) \\Rightarrow \\prod\\limits_{i=1}^n p(y_i|a(x_i, w)) \\to \\max\\limits_{w} \\Leftrightarrow \\sum\\limits_{i=1}^n|y_i - a(x_i, w)| \\to \\min\\limits_{w} \\text{(MAE)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомнила я, что в прошлом семестре проверяла лабу на похожую тему у третьекурсников, и решила заглянуть в семинар по ней. В нем действительно оказалось то, что нужно, прикладываю ссылку https://github.com/esokolov/ml-course-msu/blob/master/ML17/lecture-notes/sem20-glm.pdf\n",
    "Там действительно все очень подробно описывается и выводится, так что здесь приведу \"сухой остаток\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Пусть у нас задача бинарной классификации, $y_i \\in \\{0, 1\\}$, алгоритм $a(x, w) = \\mathbb{P}(class(x) = 1)$\n",
    "\n",
    "$$p(y|a(x, w)) = Bernoulli(y|a(x, w)) = a(x, w)^y (1 - a(x, w))^{1 - y} \\Rightarrow \\prod_{i=1}^n p(y_i|a(x_i, w)) \\to \\max\\limits_{w} \\Leftrightarrow$$\n",
    "$$\\sum_{i=1}^n y_i \\log(a(x_i, w)) + (1 - y_i) \\log(1 - a(x_i, w)) \\to \\max\\limits_{w} \\Leftrightarrow \n",
    "\\sum_{i=1}^n logloss(y_i, y_i^{pred}) \\to \\min_{w}$$\n",
    ", где $y_i^{pred} = a(x_i, w)$, $logloss(y^{true}, y^{pred}) = y^{true}\\log(y^{pred}) + (1 - y^{true})\\log(1 - y^{pred})$\n",
    "\n",
    "В случае линейного классификатора $b(x, w) = w^Tx + w_0, a(x, w)$ получается равным (в pdf это выводится) $a(x, w) = \\sigma(b(x, w))$, где $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$. Или, следуя логике, можно сказать \"нам нужно, чтобы линейный классификатор выдавал вероятности, о, навесить сигмоиду над линейной функцией нам поможет\" (это объяснение для совсем маленьких:)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Вставлю сюда и про распределение Пуассона, хотя функция потерь получается какая-то безымянная. Пусть $y \\in \\mathbb{N}$, решаем задачу линейной регрессии.\n",
    "$$p(y|a(x, w)) = Pois(y|a(x, w)) = \\dfrac{e^{-a(x, w)}a(x,w)^y}{y!} \\Rightarrow \\prod_{i=1}^n p(y_i|a(x_i, w)) \\to \\max\\limits_{w} \\Leftrightarrow$$\n",
    "$$\\sum\\limits_{i=1}^n -a(x_i,w) + y_i\\log(a(x_i, w)) + const(w) \\to \\max\\limits{w} \\Leftrightarrow \\mathcal{L}(y, a(x, w)) = \\sum\\limits_{i=1}^m a(x_i, w) - y_i\\log(a(x_i, w)) \\to \\min\\limits_{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** Вспомнила, что еще есть $hingeloss(y^{true}, y^{pred}) = max(1 - y^{true}y^{pred}, 0)$, где $y^{true} \\in \\{-1, 1\\}$ и решила поискать какие-то вероятностные аналоги вывода SVM. Нашла вот это https://arxiv.org/pdf/1707.05532.pdf\n",
    "Там фишка в том, что вводится псевдо-правдоподобие (оно ненормированное), в которую фактически в степень экспоненты запихивают cам hingle-loss, который, естественно при дифференцировании выродится в просто в оптимизацию самого hinge-loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
